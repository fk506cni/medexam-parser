### Step 3: 問題チャンク分割 (LLM利用)

1.  **初期実装と課題**:
    *   当初、正規表現による問題チャンク分割を試みたが、実際のPDFテキスト（特に問題番号の形式）とマッチせず、問題が抽出できなかった。
    *   ユーザーからの指示により、LLM（`gemini-1.5-flash`相当）を利用したチャンク分割に方針転換。
2.  **LLMプロンプト設計**:
    *   実際のPDFテキスト（`step2_reordered_text.txt`）を調査し、問題番号が「問題 XX」ではなく「 XX」のように行頭の数字のみで始まることを確認。
    *   この知見に基づき、LLMに問題の開始点を正確に認識させるためのプロンプトを再設計。役割、タスク、出力形式、具体例、重要事項を明確に定義。
    *   プロンプト内のJSON例の波括弧 `{}` がPythonの`.format()`で誤って解釈される問題が発生。
    *   **解決策**: プロンプト内のJSON例の波括弧を二重 `{{ }}` にエスケープすることで解決。
3.  **プロンプトの外部ファイル化**:
    *   プロンプトの管理性を高めるため、`src/steps/step3_prompt.txt`として外部ファイルに分離。`step3_chunk.py`からこのファイルを読み込むように変更。
4.  **長文テキスト対応（チャンク処理）**:
    *   LLMの入力トークン制限や、将来的な長文PDFへの対応を考慮し、入力テキストをオーバーラップするチャンクに分割してLLMに送信するロジックを実装。
    *   LLMからの各チャンクの出力を統合し、重複する問題を問題番号で管理して最終的な問題リストを生成する。
5.  **LLM応答のパース改善**:
    *   LLMがJSON出力の際に ````json`...```` のようなMarkdownコードブロックで囲んでくる場合があるため、`step3_chunk.py`のJSON抽出正規表現を強化し、柔軟にJSON配列を抽出できるように修正。

### Step 3.5: 機能改善と安定化

1.  **LLM APIレート制限対応**:
    *   **課題**: LLM APIには通常、短時間に大量のリクエストを送信するとエラーになるレート制限が存在する。`step3_chunk.py`のチャンクごとの連続API呼び出しがこれに抵触する可能性があった。
    *   **解決策**: `main.py`に`--rate-limit-wait`コマンドライン引数を追加。`step3_chunk.py`のAPI呼び出しループにこの待機時間（デフォルト10秒）を適用し、安定性を向上させた。

2.  **画像主体PDFの処理分離**:
    *   **課題**: 問題文PDF (`..._01.pdf`) と画像PDF (`..._02.pdf`) が分離されている形式に対応する必要があった。画像PDFにテキスト処理（Step 2, 3）を実行すると、不要な処理やエラーの原因となる。
    *   **解決策**: `main.py`でファイル名をチェックし、`_02.pdf`で終わるファイルの場合は画像主体と判断。テキスト処理が前提となるStep 2とStep 3を自動的にスキップするロジックを追加した。

3.  **LLMモデルの柔軟な選択**:
    *   **課題**: 将来的に異なるモデル（より高精度なモデルや、特定のタスクに特化したモデル）を試す際に、コードの変更なしで対応できる柔軟性が求められた。
    *   **解決策**: `main.py`に`--model-name`引数を追加し、使用するLLMモデルをコマンドラインから指定できるようにした。デフォルトは`gemini-2.5-flash-lite`に設定。`gemini-1.5-flash-latest`ではないので注意。

4.  **エラーハンドリング強化**:
    *   **課題**: LLM APIが一度も成功せずに`step3_chunk.py`が終了した場合に、`UnboundLocalError`が発生していた。
    *   **解決策**: API呼び出しループの前で結果を格納する`chunks`変数を初期化することで、APIが一度も成功しなかった場合でもスクリプトが正常に終了するように修正した。
