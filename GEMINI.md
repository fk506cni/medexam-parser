### Step 3: 問題チャンク分割 (LLM利用)

1.  **初期実装と課題**:
    *   当初、正規表現による問題チャンク分割を試みたが、実際のPDFテキスト（特に問題番号の形式）とマッチせず、問題が抽出できなかった。
    *   ユーザーからの指示により、LLM（`gemini-1.5-flash`相当）を利用したチャンク分割に方針転換。
2.  **LLMプロンプト設計**:
    *   実際のPDFテキスト（`step2_reordered_text.txt`）を調査し、問題番号が「問題 XX」ではなく「 XX」のように行頭の数字のみで始まることを確認。
    *   この知見に基づき、LLMに問題の開始点を正確に認識させるためのプロンプトを再設計。役割、タスク、出力形式、具体例、重要事項を明確に定義。
    *   プロンプト内のJSON例の波括弧 `{}` がPythonの`.format()`で誤って解釈される問題が発生。
    *   **解決策**: プロンプト内のJSON例の波括弧を二重 `{{ }}` にエスケープすることで解決。
3.  **プロンプトの外部ファイル化**:
    *   プロンプトの管理性を高めるため、`src/steps/step3_prompt.txt`として外部ファイルに分離。`step3_chunk.py`からこのファイルを読み込むように変更。
4.  **長文テキスト対応（チャンク処理）**:
    *   LLMの入力トークン制限や、将来的な長文PDFへの対応を考慮し、入力テキストをオーバーラップするチャンクに分割してLLMに送信するロジックを実装。
    *   LLMからの各チャンクの出力を統合し、重複する問題を問題番号で管理して最終的な問題リストを生成する。
5.  **LLM応答のパース改善**:
    *   LLMがJSON出力の際に ````json`...```` のようなMarkdownコードブロックで囲んでくる場合があるため、`step3_chunk.py`のJSON抽出正規表現を強化し、柔軟にJSON配列を抽出できるように修正。

### Step 4: 問題の構造化 (LLM利用)

1.  **実装アプローチ**:
    *   Step 3で生成された問題チャンクを、問題文、選択肢などを持つ構造化JSONに変換するため、LLMを利用するアプローチを採用。
    *   APIのレート制限に対応し、かつ効率的に処理を行うため、複数の問題をまとめて処理する**バッチ処理**を導入。

2.  **プロンプトの改善サイクル**:
    *   **初期プロンプト**: `README.md`の出力例を参考にプロンプトを作成したが、LLMが実際の入力データを処理せず、プロンプト内の出力例をそのまま返してしまう問題が発生。
    *   **原因分析**: LLMへの指示が曖昧で、実際の入力データ（`step3_problem_chunks.json`）の形式とプロンプト内の入力例が異なっていたため、LLMがタスクを誤解したと判断。
    *   **プロンプト修正**: 指示を「提供されたデータを変換せよ」とより明確化し、入力例も実際のデータ形式に合わせた。しかし、Pythonの`.format()`がプロンプト内の波括弧を意図せず解釈しようとする`KeyError`が繰り返し発生。
    *   **最終的な解決策**: `.format()`の使用をやめ、単純な文字列置換（`.replace()`）でプロンプトを組み立てるように変更。これにより、意図しないフォーマットエラーを完全に回避し、LLMが指示通りに動作するようになった。

3.  **デバッグ効率化**:
    *   **課題**: 大量の問題を一度に処理すると、プロンプトの修正やデバッグのサイクルに時間がかかっていた。
    *   **解決策**: `main.py`に`--max-batches`引数を追加。これにより、処理するバッチ数を制限し、少数のデータで迅速に動作確認を行えるように改善した。
