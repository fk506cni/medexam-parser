### Step 3: 問題チャンク分割 (LLM利用)

1.  **初期実装と課題**:
    *   当初、正規表現による問題チャンク分割を試みたが、実際のPDFテキスト（特に問題番号の形式）とマッチせず、問題が抽出できなかった。
    *   ユーザーからの指示により、LLM（`gemini-1.5-flash`相当）を利用したチャンク分割に方針転換。
2.  **LLMプロンプト設計**:
    *   実際のPDFテキスト（`step2_reordered_text.txt`）を調査し、問題番号が「問題 XX」ではなく「 XX」のように行頭の数字のみで始まることを確認。
    *   この知見に基づき、LLMに問題の開始点を正確に認識させるためのプロンプトを再設計。役割、タスク、出力形式、具体例、重要事項を明確に定義。
    *   プロンプト内のJSON例の波括弧 `{}` がPythonの`.format()`で誤って解釈される問題が発生。
    *   **解決策**: プロンプト内のJSON例の波括弧を二重 `{{ }}` にエスケープすることで解決。
3.  **プロンプトの外部ファイル化**:
    *   プロンプトの管理性を高めるため、`src/steps/step3_prompt.txt`として外部ファイルに分離。`step3_chunk.py`からこのファイルを読み込むように変更。
4.  **長文テキスト対応（チャンク処理）**:
    *   LLMの入力トークン制限や、将来的な長文PDFへの対応を考慮し、入力テキストをオーバーラップするチャンクに分割してLLMに送信するロジックを実装。
    *   LLMからの各チャンクの出力を統合し、重複する問題を問題番号で管理して最終的な問題リストを生成する。
5.  **LLM応答のパース改善**:
    *   LLMがJSON出力の際に ````json`...```` のようなMarkdownコードブロックで囲んでくる場合があるため、`step3_chunk.py`のJSON抽出正規表現を強化し、柔軟にJSON配列を抽出できるように修正。